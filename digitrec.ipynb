{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "digitrec.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1HPmkNS36wEv",
        "Io-WhKnHUh6P"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fszgXmA8MZY"
      },
      "source": [
        "**EIE558 Speech Recognition Lab (Part 1): Spoken-Digit Recognition**\n",
        "\n",
        "In this lab, you will train and evaluate a CNN model that comprises several 1-D CNN layers for spoken digit recognition. By default, the input to the CNN is an MFCC matrix of size *C* x *T*, where *C* is the number MFCC coefficients per frame and *T* is the number of frames. \n",
        "\n",
        "Two pooling methods are available for converting frame-based features to utterance-based features. They are adaptive average pooling and statistics pooling. The former uses Pytorch's AdaptiveAvgPooling2d() to average the last convolutional layer's activation across the frame axis. The latter concatenates the mean and the standard deviation of the activations across frames, which is commonly used in the x-vector network. If no pooling method is used, the number of frames for each utterance should be the same so that the number of nodes after flattening is identical for all utterances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "186SdUa78s9A"
      },
      "source": [
        "<font color=\"green\">*Step 1: Install PyTorch*<font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzQNackhq703",
        "outputId": "4de06f44-4262-4af7-c281-df4199f07449"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!mkdir -p /content/drive/MyDrive/Learning/EIE558\n",
        "%cd /content/drive/MyDrive/Learning/EIE558/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Learning/EIE558\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wkb5wk6gMdZ6"
      },
      "source": [
        "# Create working directory. Ignore this step if 'EIE558' directory is existing. \n",
        "!mkdir -p /content/drive/MyDrive/Learning/EIE558"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kKrmmkAMkSp"
      },
      "source": [
        "# Go to working directory.\n",
        "%cd /content/drive/MyDrive/Learning/EIE558/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atQxZRpru9fu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc142b54-75de-40d5-8df4-764bef9158a0"
      },
      "source": [
        "# Make sure that GPU will be used by clicking \"Edit\" --> \"Notebook Setting\"\n",
        "!pip3 install torch==1.5.1 torchaudio==0.5 -f https://download.pytorch.org/whl/cu101/torch_stable.html"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/cu101/torch_stable.html\n",
            "Collecting torch==1.5.1\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp37-cp37m-linux_x86_64.whl (704.4MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4MB 25kB/s \n",
            "\u001b[?25hCollecting torchaudio==0.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/22/f9b9448cd7298dbe2adb428a1527dd4b3836275337da6f34da3efcd12798/torchaudio-0.5.0-cp37-cp37m-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.5.1) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.5.1) (1.19.5)\n",
            "\u001b[31mERROR: torchvision 0.9.0+cu101 has requirement torch==1.8.0, but you'll have torch 1.5.1+cu101 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.0 has requirement torch==1.8.0, but you'll have torch 1.5.1+cu101 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchaudio 0.5.0 has requirement torch==1.5.0, but you'll have torch 1.5.1+cu101 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchaudio\n",
            "  Found existing installation: torch 1.8.0+cu101\n",
            "    Uninstalling torch-1.8.0+cu101:\n",
            "      Successfully uninstalled torch-1.8.0+cu101\n",
            "Successfully installed torch-1.5.1+cu101 torchaudio-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41E_3gSuyScL"
      },
      "source": [
        "<font color=\"green\">*Step 2: Download data*<font>. <font color=\"red\">*In case the website http://bioinfo.eie.polyu.edu.hk is too slow or busy, you may find the files [here](https://polyuit-my.sharepoint.com/:f:/g/personal/enmwmak_polyu_edu_hk/EpX3v5ykT_VLoiBa8jrpJ70B52X4XbEPQcyrDnLAquEcIA?e=d5Xjrv)<font>*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh-jUQpD7gde",
        "outputId": "aca10dc6-2d3c-4112-a7a4-5fa42b5dd09f"
      },
      "source": [
        "# Download dataset. If the 'data' directory exists and is empty, \n",
        "# you may delete the 'data' directory and run this step again.\n",
        "%%shell\n",
        "pwd\n",
        "dir=\"python-asr\" \n",
        "if [ ! -d $dir ]; then\n",
        "  echo \"Directory $dir does not exist. Downloading ${dir}.tgz\"\n",
        "  wget http://bioinfo.eie.polyu.edu.hk/download/EIE558/asr/${dir}.tgz;\n",
        "  unzip -o ${dir}.tgz;\n",
        "  rm -f ${dir}.tgz*;\n",
        "else\n",
        "  echo \"Directory $dir already exist\"\n",
        "fi"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Learning/EIE558\n",
            "Directory python-asr already exist\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzoJNd6PmKt8",
        "outputId": "74593b6c-cf35-418d-c455-fa6bf3358cfa"
      },
      "source": [
        "%cd /content/drive/MyDrive/Learning/EIE558/python-asr/\n",
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Learning/EIE558/python-asr\n",
            "data  digitrec.py  model.py  models  __pycache__  short_test.lst  sphrec.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-ToOSCe6zej"
      },
      "source": [
        "%%shell\n",
        "dir=\"data\" \n",
        "if [ ! -d $dir ]; then\n",
        "  echo \"Directory $dir does not exist. Downloading ${dir}.zip\"\n",
        "  wget http://bioinfo.eie.polyu.edu.hk/download/EIE558/asr/${dir}.zip;\n",
        "  unzip -o ${dir}.zip;\n",
        "  rm -f ${dir}.zip*;\n",
        "else\n",
        "  echo \"Directory $dir already exist\"\n",
        "fi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub383ypr9D5n"
      },
      "source": [
        "<font color=\"green\">*Step 3: Train a CNN model. It may take several hours to train a model if you use all of the training data in the list file \"data/digits/train.lst\". You may want to use the pre-trained models in the folder \"models/\" if you want to obtain test accuracy only. Read the file \"digitrec.py\" and \"model.py\" to see how to implement a CNN for spoken digit recognition. If you want to train your own models, you may modify the file \"digitrec.py such that \"data/digits/train.lst\" is replaced by \"data/digits/short_train.lst\" and \"data/digits/test.lst\" is replaced by data/digits/short_test.lst\". With these modifications, it will take about 30 minutes to train a network. But the accuracy is lower.*</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkf5va9trFc_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "388579f7-70b5-436b-be4e-0f754e025c3e"
      },
      "source": [
        "%cd /content/drive/MyDrive/Learning/EIE558/python-asr\n",
        "!more data/digits/train.lst | sed -n '1,2000p' > data/digits/short_train.lst\n",
        "!more data/digits/test.lst | sed -n '1,500p' > data/digits/short_test.lst\n",
        "!mkdir -p models/mymodels\n",
        "!python3 digitrec.py --pool_method stats --model_file models/mymodels/spokendigit_cnn_stats.pth"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Learning/EIE558/python-asr\n",
            "Epoch  0\n",
            "100% 32/32 [01:04<00:00,  2.03s/it]\n",
            "Last lr:  0.00027089460458501675  Train_loss:  2.319438934326172  Val_loss:  2.272307872772217  Accuracy: 8.62%\n",
            "Epoch  1\n",
            "100% 32/32 [01:04<00:00,  2.03s/it]\n",
            "Last lr:  0.0007554032818386438  Train_loss:  2.190788745880127  Val_loss:  2.1043598651885986  Accuracy: 10.82%\n",
            "Epoch  2\n",
            "100% 32/32 [01:04<00:00,  2.02s/it]\n",
            "Last lr:  0.001  Train_loss:  1.9589784145355225  Val_loss:  1.9117239713668823  Accuracy: 39.63%\n",
            "Epoch  3\n",
            "100% 32/32 [01:05<00:00,  2.03s/it]\n",
            "Last lr:  0.0009504846320134736  Train_loss:  1.8161309957504272  Val_loss:  1.8114361763000488  Accuracy: 59.90%\n",
            "Epoch  4\n",
            "100% 32/32 [01:05<00:00,  2.05s/it]\n",
            "Last lr:  0.000811745653949763  Train_loss:  1.7427036762237549  Val_loss:  1.756650686264038  Accuracy: 66.98%\n",
            "Epoch  5\n",
            "100% 32/32 [01:05<00:00,  2.04s/it]\n",
            "Last lr:  0.0006112620219362892  Train_loss:  1.7039320468902588  Val_loss:  1.7252854108810425  Accuracy: 73.75%\n",
            "Epoch  6\n",
            "100% 32/32 [01:04<00:00,  2.03s/it]\n",
            "Last lr:  0.00038874197806371076  Train_loss:  1.665127158164978  Val_loss:  1.704300045967102  Accuracy: 76.55%\n",
            "Epoch  7\n",
            "100% 32/32 [01:05<00:00,  2.03s/it]\n",
            "Last lr:  0.00018825834605023698  Train_loss:  1.6447911262512207  Val_loss:  1.6978769302368164  Accuracy: 76.92%\n",
            "Epoch  8\n",
            "100% 32/32 [01:05<00:00,  2.04s/it]\n",
            "Last lr:  4.9519367986526286e-05  Train_loss:  1.635568380355835  Val_loss:  1.7009438276290894  Accuracy: 75.75%\n",
            "Epoch  9\n",
            "100% 32/32 [01:06<00:00,  2.09s/it]\n",
            "Last lr:  4e-09  Train_loss:  1.6257158517837524  Val_loss:  1.696245789527893  Accuracy: 77.27%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4_dg4YTBuFk"
      },
      "source": [
        "<font color=\"green\">*Step 4: Load the trained model (or the pre-trained model) and evaluate it.*</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfIDu7n4j7q7",
        "outputId": "1c8325eb-6d81-4047-d83b-18f237f71ae2"
      },
      "source": [
        "%cd /content/drive/MyDrive/Learning/EIE558/python-asr\n",
        "!ls -F data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Learning/EIE558/python-asr\n",
            "digits/  noise/  speech/  text/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdsBM4z3B64-",
        "outputId": "c2073cff-386c-42f7-9ea0-dba36ec67bcb"
      },
      "source": [
        "# load model. The example below is a pretrained model using adaptive average pooling.\n",
        "%cd /content/drive/MyDrive/Learning/EIE558/python-asr\n",
        "!ls models\n",
        "from model import CNNModel\n",
        "import torch\n",
        "DEVICE = torch.device('cuda')\n",
        "model = CNNModel(pool_method='adapt').to(DEVICE)\n",
        "model.load_state_dict(torch.load('models/spokendigit_cnn_adapt.pth'))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Learning/EIE558/python-asr\n",
            "mymodels\t\t   spokendigit_cnn_none.pth\n",
            "spokendigit_cnn_adapt.pth  spokendigit_cnn_stats.pth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKLgd6c2SmZy"
      },
      "source": [
        "@torch.no_grad()\n",
        "def predict_dl(model, dl):\n",
        "    torch.cuda.empty_cache()\n",
        "    batch_probs = []\n",
        "    batch_targ = []\n",
        "    for xb, yb in dl:\n",
        "        xb = xb.float().to(torch.device('cuda'))\n",
        "        yb = yb.float().to(torch.device('cuda'))\n",
        "        probs = model(xb)\n",
        "        batch_probs.append(probs.cpu().detach())\n",
        "        batch_targ.append(yb.cpu().detach())\n",
        "    batch_probs = torch.cat(batch_probs)\n",
        "    batch_targ = torch.cat(batch_targ)\n",
        "    return [list(values).index(max(values)) for values in batch_probs], batch_targ"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_y7KtoGpPLU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1023eda-10d6-4932-af4e-7d516c8d4a6a"
      },
      "source": [
        "!more data/digits/test.lst | sed -n '1,500p' > data/digits/short_test.lst \n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from digitrec import SpeechDataset, evaluate\n",
        "test_set = SpeechDataset(filelist='data/digits/short_test.lst', rootdir='data/digits', n_mfcc=20)\n",
        "test_dl = DataLoader(test_set, batch_size=64, shuffle=False, num_workers=16, pin_memory=True)\n",
        "r = evaluate(model, test_dl)\n",
        "yp, yt = predict_dl(model, test_dl)\n",
        "print(\"Loss: \", r['loss'], \"\\nAccuracy: \", r['accuracy'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  1.737992286682129 \n",
            "Accuracy:  0.6930589079856873\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG5WjvpDNiPn"
      },
      "source": [
        "# load the model that was trained on the trimed dataset. The example below is\n",
        "# a model using statistics pooling in its embedding layer.\n",
        "%cd /content/drive/MyDrive/Learning/EIE558/python-asr\n",
        "!ls models\n",
        "from model import CNNModel\n",
        "import torch\n",
        "DEVICE = torch.device('cuda')\n",
        "model = CNNModel(pool_method='adapt').to(DEVICE)\n",
        "model.load_state_dict(torch.load('models/mymodels/spokendigit_cnn_adapt.pth'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFvt-DWuN9YA"
      },
      "source": [
        "!more data/digits/test.lst | sed -n '1,500p' > data/digits/short_test.lst \n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from digitrec import SpeechDataset, evaluate\n",
        "test_set = SpeechDataset(filelist='data/digits/short_test.lst', rootdir='data/digits', n_mfcc=20)\n",
        "test_dl = DataLoader(test_set, batch_size=64, shuffle=False, num_workers=16, pin_memory=True)\n",
        "r = evaluate(model, test_dl)\n",
        "yp, yt = predict_dl(model, test_dl)\n",
        "print(\"Loss: \", r['loss'], \"\\nAccuracy: \", r['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nudpBj4Zkemr"
      },
      "source": [
        "<font color=\"green\">*Step 5: Varying the kernel size. Increase the kernel size in \"model.py\" to 7 (or even larger) and repeat Step 4 and Step 5. Record the test loss and accuracy. Reduce the kernel size to 1 and observe the results. Can the CNN still capture the temporal characteristics in the MFCCs when kernel_size=1? Explain your answer.*</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNudrUfu49vW"
      },
      "source": [
        "<font color=\"green\">*Step 6: Reduce the depth of the network so that the conv2, conv3, and conv4 in \"model.py\" are removed. After the change, the network only have one convolutional layer. Observe the performance of the network. Note that large and deep networks may not necessary produce better results, especially when the amount of training data is limited.*</font>"
      ]
    }
  ]
}